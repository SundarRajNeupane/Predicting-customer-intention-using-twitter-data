{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d515af",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/training.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-95dd9a8496ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m stats, test_data, output_data_frame = output_to_results(\"data/training.csv\",\n\u001b[0m\u001b[0;32m    281\u001b[0m                                                         \"data/test.csv\", \"TF\", \"Neural Network\")\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-95dd9a8496ae>\u001b[0m in \u001b[0;36moutput_to_results\u001b[1;34m(pathData_train, pathData_test, doc_vector, model)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0moutput_to_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpathData_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathData_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[0moutput_data_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m     \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_undefined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpathData_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m     \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data_undefined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpathData_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[0moutput_data_frame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweets'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-95dd9a8496ae>\u001b[0m in \u001b[0;36mextract\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mfd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mdefined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"undefined\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/training.csv'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    ")\n",
    "\n",
    "\n",
    "def extract(path):\n",
    "    fd = open(path, encoding=\"utf-8\", errors=\"replace\")\n",
    "    df = pd.read_csv(fd)\n",
    "    defined = df[\"class\"] != (\"undefined\")\n",
    "    # #output dataframe without undeined\n",
    "    df2 = df[defined]\n",
    "    defined1 = df2[\"class\"] != \"Undefined\"\n",
    "    df4 = df2[defined1]\n",
    "    # replace no PI with no\n",
    "    df3 = df4.replace(\"No PI\", \"no\")\n",
    "    # replace PI with yes\n",
    "    final = df3.replace(\"PI\", \"yes\")\n",
    "    replace_yes = final.replace(\"Yes\", \"yes\")\n",
    "    final_df = replace_yes.replace(\"No\", \"no\")\n",
    "    return final_df, df\n",
    "\n",
    "\n",
    "def handle_negation(final_df):\n",
    "    out_df = pd.DataFrame()\n",
    "    count_tweet = 0\n",
    "    for text in final_df['text']:\n",
    "        temp_text = \"\"\n",
    "        li_text = text.split()\n",
    "        for word in li_text:\n",
    "            count = 0\n",
    "            lower_word = word.lower()\n",
    "            if lower_word == \"didn't\" or lower_word == \"not\" or lower_word == \"no\" or lower_word == \"never\"\\\n",
    "                    or lower_word == \"don't\":\n",
    "                temp = count + 1\n",
    "                temp_text = temp_text + word + \" \"\n",
    "                for i in range(temp, len(li_text)):\n",
    "                    if li_text[i] in [\",\", \"?\", \"!\", \".\"]:\n",
    "                        temp_text = \" \"+temp_text + li_text[i] + \" \"\n",
    "                        break\n",
    "                    else:\n",
    "                        temp_text = temp_text + \"NOT_\" + li_text[i]+\" \"\n",
    "\n",
    "            else:\n",
    "                temp_text = temp_text + word + \" \"\n",
    "        # print(temp_text)\n",
    "        out_df.at[count_tweet, 'text'] = temp_text\n",
    "        out_df.at[count_tweet, 'class'] = final_df.iloc[count_tweet]['class']\n",
    "        count_tweet += 1\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def space(final_df):\n",
    "    new_df = pd.DataFrame()\n",
    "    count_tweets = 0\n",
    "    for text in final_df['text']:\n",
    "        temp = \"\"\n",
    "        for char in text:\n",
    "            if char in [\",\", \".\", \"!\", \"?\", \":\", \";\"]:\n",
    "                temp = temp + ' ' + char\n",
    "\n",
    "            else:\n",
    "                temp = temp + char\n",
    "        # print(temp)\n",
    "        new_df.at[count_tweets, 'text'] = temp\n",
    "        new_df.at[count_tweets, 'class'] = final_df.iloc[count_tweets]['class']\n",
    "        count_tweets += 1\n",
    "    # print(\"new_df\")\n",
    "    # print(new_df)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def report_results(model, X, y):\n",
    "    # print(X.shape)\n",
    "    # print(y.shape)\n",
    "    pred_proba = model.predict_proba(X)[:, 1]\n",
    "    pred = model.predict(X)\n",
    "    auc = roc_auc_score(y, pred_proba)\n",
    "    acc = accuracy_score(y, pred)\n",
    "    f1 = f1_score(y, pred)\n",
    "    prec = precision_score(y, pred)\n",
    "    rec = recall_score(y, pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    TrueNeg = tn / (tn + fp)\n",
    "    result = {\n",
    "        \"auc\": auc,\n",
    "        \"f1\": f1,\n",
    "        \"acc\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn,\n",
    "        \"TP\": tp,\n",
    "        \"True Negative rate\": TrueNeg,\n",
    "    }\n",
    "    return result, pred, pred_proba, acc, TrueNeg\n",
    "\n",
    "\n",
    "def data_preprocessing(final_data_frame):\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Data cleaning step wise\n",
    "    # -----------------------------------------------------------------------\n",
    "    # 1. LOWERCASE\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(\n",
    "        lambda x: \" \".join(x.lower() for x in x.split())\n",
    "    )\n",
    "    # NEGATION HANDLING\n",
    "    final_data_frame = space(final_data_frame)\n",
    "    final_data_frame = handle_negation(final_data_frame)\n",
    "    # 2. REMOVE PUNC\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].str.replace(\n",
    "        \"[^\\w\\s]\", \"\")\n",
    "    # 3. STOPWORDS REMOVAL\n",
    "    stop = stopwords.words(\"english\")\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(\n",
    "        lambda x: \" \".join(x for x in x.split() if x not in stop)\n",
    "    )\n",
    "    # 4. COMMON WORD REMOVAL\n",
    "    freq = pd.Series(\n",
    "        \" \".join(final_data_frame[\"text\"]).split()).value_counts()[:2]\n",
    "    freq = list(freq.index)\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(\n",
    "        lambda x: \" \".join(x for x in x.split() if x not in freq)\n",
    "    )\n",
    "    # 5. RARE WORDS REMOVAL\n",
    "    rare = pd.Series(\n",
    "        \" \".join(final_data_frame[\"text\"]).split()).value_counts()[-10:]\n",
    "    rare = list(rare.index)\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(\n",
    "        lambda x: \" \".join(x for x in x.split() if x not in rare)\n",
    "    )\n",
    "    # 6. SPELLING CORRECTION\n",
    "    final_data_frame[\"text\"][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "    # 7. STEMMING\n",
    "    st = PorterStemmer()\n",
    "    final_data_frame[\"text\"][:5].apply(\n",
    "        lambda x: \" \".join([st.stem(word) for word in x.split()])\n",
    "    )\n",
    "    # 8. LEMMATIZATION\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(\n",
    "        lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])\n",
    "    )\n",
    "    # -----------------------------------------------------------------------\n",
    "    # BUILDING THE CORPUS\n",
    "    # -----------------------------------------------------------------------\n",
    "    corpus = []\n",
    "    for text in final_data_frame[\"text\"]:\n",
    "        corpus.append(text)\n",
    "    # -----------------------------------------------------------------------\n",
    "    # CHANGE CLASS VALUES FROM YES/NO TO 0/1\n",
    "    # -----------------------------------------------------------------------\n",
    "    final_data_frame.rename(columns={\"class\": \"class_label\"}, inplace=True)\n",
    "    Class_Label = {\"yes\": 1, \"no\": 0}\n",
    "    final_data_frame.class_label = [\n",
    "        Class_Label[item] for item in final_data_frame.class_label\n",
    "    ]\n",
    "    final_data_frame.rename(columns={\"class_label\": \"class\"}, inplace=True)\n",
    "    return final_data_frame, corpus\n",
    "\n",
    "\n",
    "def output_to_results(pathData_train, pathData_test, doc_vector, model):\n",
    "    output_data_frame = pd.DataFrame()\n",
    "    train_data, train_data_undefined = extract(pathData_train)\n",
    "    test_data, test_data_undefined = extract(pathData_test)\n",
    "    output_data_frame['tweets'] = test_data['text']\n",
    "    train_data, train_corpus = data_preprocessing(train_data)\n",
    "    test_data, test_corpus = data_preprocessing(test_data)\n",
    "    output_data_frame['processed tweets'] = test_data['text']\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Select Document vector\n",
    "    # -----------------------------------------------------------------------\n",
    "    if doc_vector == \"TF\":\n",
    "        count_vectorizer = CountVectorizer()\n",
    "        count_vectorized_data_train = count_vectorizer.fit_transform(\n",
    "            train_corpus)\n",
    "        vectorized_data_train = count_vectorized_data_train\n",
    "        # count_vectorized_data_test = count_vectorizer.fit_transform(test_corpus)\n",
    "        count_vectorized_data_test = count_vectorizer.transform(test_corpus)\n",
    "        vectorized_data_test = count_vectorized_data_test\n",
    "    elif doc_vector == \"TF-IDF\":\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_vectorized_data_train = tfidf_vectorizer.fit_transform(\n",
    "            train_corpus)\n",
    "        vectorized_data_train = tfidf_vectorized_data_train\n",
    "        tfidf_vectorized_data_test = tfidf_vectorizer.transform(test_corpus)\n",
    "        vectorized_data_test = tfidf_vectorized_data_test\n",
    "    # -----------------------------------------------------------------------\n",
    "    # training/testing\n",
    "    # -----------------------------------------------------------------------\n",
    "    X_train = vectorized_data_train\n",
    "    X_test = vectorized_data_test\n",
    "    Y_train = train_data[\"class\"].values\n",
    "    Y_test = test_data[\"class\"].values\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Select model to train and display stats\n",
    "    # -----------------------------------------------------------------------\n",
    "    if model == \"SVM\":\n",
    "        SVM = svm.SVC(probability=True, C=1.0,\n",
    "                      kernel=\"linear\", degree=3, gamma=\"auto\")\n",
    "        SVM.fit(X_train, Y_train)\n",
    "\n",
    "        stats, pred, pred_proba, acc, trueNeg = report_results(\n",
    "            SVM, X_test, Y_test)\n",
    "\n",
    "    elif model == \"Naive Bayes\":\n",
    "        Naive = naive_bayes.MultinomialNB()\n",
    "        Naive.fit(X_train, Y_train)\n",
    "\n",
    "        stats, pred, pred_proba, acc, trueNeg = report_results(\n",
    "            Naive, X_test, Y_test)\n",
    "\n",
    "    elif model == \"Logistic Regression\":\n",
    "        logisticReg = linear_model.LogisticRegression(C=1.0)\n",
    "        logisticReg.fit(X_train, Y_train)\n",
    "\n",
    "        stats, pred, pred_proba, acc, trueNeg = report_results(\n",
    "            logisticReg, X_test, Y_test)\n",
    "\n",
    "    elif model == \"Decision Tree\":\n",
    "        DTC = DecisionTreeClassifier(min_samples_split=7, random_state=252)\n",
    "        DTC.fit(X_train, Y_train)\n",
    "\n",
    "        stats, pred, pred_proba, acc, trueNeg = report_results(\n",
    "            DTC, X_test, Y_test)\n",
    "\n",
    "    elif model == \"Neural Network\":\n",
    "        neural_network = MLPClassifier(\n",
    "            solver=\"lbfgs\", alpha=1e-5, hidden_layer_sizes=(20, 10, 10, 5), random_state=1\n",
    "        )\n",
    "        neural_network.fit(X_train, Y_train)\n",
    "\n",
    "        stats, pred, pred_proba, acc, trueNeg = report_results(\n",
    "            neural_network, X_test, Y_test)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    test_data['Predicted Class'] = pred.tolist()\n",
    "    output_data_frame['true class'] = test_data['class']\n",
    "    output_data_frame['prediced class'] = pred.tolist()\n",
    "    output_data_frame['score assigned by model'] = pred_proba.tolist()\n",
    "    test_data['score'] = pred_proba.tolist()\n",
    "    # print(test_data['score'])\n",
    "    test_data['class'].replace(0, \"no\", inplace=True)\n",
    "    test_data['class'].replace(1, \"yes\", inplace=True)\n",
    "    test_data['Predicted Class'].replace(0, \"no\", inplace=True)\n",
    "    test_data['Predicted Class'].replace(1, \"yes\", inplace=True)\n",
    "    #print(test_data['Predicted Class'])\n",
    "\n",
    "    output_data_frame['true class'].replace(0, \"no\", inplace=True)\n",
    "    output_data_frame['true class'].replace(1, \"yes\", inplace=True)\n",
    "    output_data_frame['prediced class'].replace(0, \"no\", inplace=True)\n",
    "    output_data_frame['prediced class'].replace(1, \"yes\", inplace=True)\n",
    "\n",
    "    output_data_frame['model'] = model\n",
    "    output_data_frame['docVec'] = doc_vector\n",
    "    output_data_frame['acc'] = acc\n",
    "    output_data_frame['trueNeg'] = trueNeg\n",
    "\n",
    "    return stats, test_data, output_data_frame\n",
    "\n",
    "\n",
    "stats, test_data, output_data_frame = output_to_results(\"data/training.csv\",\n",
    "                                                        \"data/test.csv\", \"TF\", \"Neural Network\")\n",
    "\n",
    "path_to_save = \"/home/hasan/Desktop/\"\n",
    "model_selection = \"NeuralNetwork TF neghandling lemmitization\"\n",
    "ext = \".csv\"\n",
    "file_name = path_to_save + model_selection + ext\n",
    "\n",
    "export = output_data_frame.to_csv(\n",
    "    r''+file_name, index=None, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b699c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
