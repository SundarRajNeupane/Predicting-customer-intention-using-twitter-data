{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45dc0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab7c5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98736848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fd668f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "import enchant\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4043d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3bca7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=\"It is. _ a Brilliant @johnlewisretail - added  iPhone   X  to basket and for the 10 seconds needed for card company to send  purchase  text code and enter it you sell to someone else.   Why not hold for a few mins  like \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f940c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c818a4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c58106d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is a brilliant johnlewisretail added iphone x to basket and for the 10 seconds needed for card company to send purchase text code and enter it you sell to someone else why not hold for a few mins like'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(s1,no_punct= True, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18cc3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5d823f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathData = r'C:\\Users\\su\\Desktop\\Major_Project_work\\data\\data.csv' \n",
    "pathStopwords= r'C:\\Users\\su\\Desktop\\Major_Project_work\\data\\stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e2b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4300174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCLean:\n",
    "\n",
    "    def extract(self, path):\n",
    "        #fd = open(path, encoding=\"utf-8\", errors='replace')\n",
    "        df = pd.read_csv(r'C:\\Users\\su\\Desktop\\Major_Project_work\\data\\data.csv')\n",
    "        defined = df['class'] != (\"undefined\")\n",
    "        # #output dataframe without undeined\n",
    "        df2 = df[defined]\n",
    "        defined1 = df2['class'] != \"Undefined\"\n",
    "        df4 = df2[defined1]\n",
    "        # replace no PI with no\n",
    "        df3 = df4.replace(\"No PI\", \"no\")\n",
    "        # replace PI with yes\n",
    "        final = df3.replace(\"PI\", \"yes\")\n",
    "\n",
    "        replace_yes = final.replace(\"Yes\", \"yes\")\n",
    "        final_df = replace_yes.replace(\"No\", \"no\")\n",
    "        return final_df, df\n",
    "\n",
    "    def text_concat(self, final_df):\n",
    "        text = \"\"\n",
    "        for x in final_df[\"text\"]:\n",
    "            text = text + str(x)\n",
    "        return text\n",
    "\n",
    "    def read_stopwords(self, path):\n",
    "        file1 = open(path, \"r\")\n",
    "        stopword = file1.readlines()\n",
    "        file1.close()\n",
    "        li_stopwords = stopword[0].split()\n",
    "        return li_stopwords\n",
    "\n",
    "    def removeStopWords(self, text):\n",
    "        # stop_words = set(stopwords.words('english'))\n",
    "        stop_words = self.read_stopwords(pathStopwords)\n",
    "        word_tokens = word_tokenize(text)\n",
    "        filtered_sentence = []\n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w.lower())\n",
    "                # return list of corpus without stop words in a list.\n",
    "        return filtered_sentence\n",
    "\n",
    "    def remove_stopwords(self, df_punc_remove):\n",
    "        # stop_words = set(stopwords.words('english'))\n",
    "        li_stopwords = self.read_stopwords(pathStopwords)\n",
    "        # print(stop_words)\n",
    "        count_clean = 0\n",
    "        for text in df_punc_remove['text']:\n",
    "            word_tokens = word_tokenize(text)\n",
    "            clean_text = \"\"\n",
    "            for w in word_tokens:\n",
    "                if w.lower() not in li_stopwords:\n",
    "                    clean_text = clean_text + w.lower() + ' '\n",
    "            df_punc_remove.at[count_clean, 'text'] = clean_text\n",
    "            df_punc_remove.at[count_clean,\n",
    "                              'class'] = df_punc_remove.iloc[count_clean]['class']\n",
    "            count_clean += 1\n",
    "        # return list of corpus without stop words in a list.\n",
    "        # print(df_punc_remove)\n",
    "        return df_punc_remove\n",
    "\n",
    "    def removePunc(self, eachText):\n",
    "        remove_punc = re.sub(r'[^\\w\\s]', '', eachText)\n",
    "        return remove_punc\n",
    "        # pattern = re.compile(r'[a-zA-Z]+')\n",
    "        # matches = pattern.finditer(eachText)\n",
    "        # new_corpus = \"\"\n",
    "        # for match in matches:\n",
    "        #     new_corpus = new_corpus + match.group() + \" \"\n",
    "        # return new_corpus\n",
    "\n",
    "    def remove_punc(self, temp_df):\n",
    "        count = 0\n",
    "        for text in temp_df['text']:\n",
    "            out = re.sub(r'[^\\w\\s]', '', text)\n",
    "            temp_df.at[count, 'text'] = out\n",
    "            temp_df.at[count, 'class'] = temp_df.iloc[count]['class']\n",
    "            count += 1\n",
    "        return temp_df\n",
    "\n",
    "    def space(self, final_df):\n",
    "        new_df = pd.DataFrame()\n",
    "        count_tweets = 0\n",
    "        for text in final_df['text']:\n",
    "            temp = \"\"\n",
    "            for char in text:\n",
    "                if char in [\",\", \".\", \"!\", \"?\", \":\", \";\"]:\n",
    "                    temp = temp + ' ' + char\n",
    "\n",
    "                else:\n",
    "                    temp = temp + char\n",
    "            # print(temp)\n",
    "            new_df.at[count_tweets, 'text'] = temp\n",
    "            new_df.at[count_tweets,\n",
    "                      'class'] = final_df.iloc[count_tweets]['class']\n",
    "            count_tweets += 1\n",
    "        # print(\"new_df\")\n",
    "        # print(new_df)\n",
    "        return new_df\n",
    "\n",
    "    def handle_negation(self, final_df):\n",
    "        out_df = pd.DataFrame()\n",
    "        count_tweet = 0\n",
    "        for text in final_df['text']:\n",
    "            temp_text = \"\"\n",
    "            li_text = text.split()\n",
    "            for word in li_text:\n",
    "                count = 0\n",
    "                lower_word = word.lower()\n",
    "                if lower_word == \"didn't\" or lower_word == \"not\" or lower_word == \"no\" or lower_word == \"never\"\\\n",
    "                        or lower_word == \"don't\" or lower_word == \"hate\":\n",
    "                    temp = count + 1\n",
    "                    temp_text = temp_text + word + \" \"\n",
    "                    for i in range(temp, len(li_text)):\n",
    "                        if li_text[i] in [\",\", \"?\", \"!\", \".\"]:\n",
    "                            temp_text = \" \"+temp_text + li_text[i] + \" \"\n",
    "                            break\n",
    "                        else:\n",
    "                            temp_text = temp_text + \"NOT_\" + li_text[i]+\" \"\n",
    "\n",
    "                else:\n",
    "                    temp_text = temp_text + word + \" \"\n",
    "            # print(temp_text)\n",
    "            out_df.at[count_tweet, 'text'] = temp_text\n",
    "            out_df.at[count_tweet, 'class'] = final_df.iloc[count_tweet]['class']\n",
    "            count_tweet += 1\n",
    "        return out_df\n",
    "\n",
    "    def check_english(self, temp_df):\n",
    "        d = enchant.Dict(\"en_US\")\n",
    "        new_eng = pd.DataFrame()\n",
    "        count = 0\n",
    "\n",
    "        for sentence in temp_df['text']:\n",
    "            temp_sent = \"\"\n",
    "            for word in sentence.split():\n",
    "                temp = word.lower()\n",
    "                if d.check(word):\n",
    "                    # print(\"ammar\")\n",
    "                    temp_sent = temp_sent + temp + \" \"\n",
    "            # print(temp_sent)\n",
    "            new_eng.at[count, 'text'] = temp_sent\n",
    "            new_eng.at[count, 'class'] = temp_df.iloc[count]['class']\n",
    "            count += 1\n",
    "        # print(new_eng)\n",
    "        return new_eng\n",
    "\n",
    "    def Stemming(self, temp_df):\n",
    "        new_eng = pd.DataFrame()\n",
    "        count = 0\n",
    "# \n",
    "        for sentence in temp_df['text']:\n",
    "            temp_sent = \"\"\n",
    "            for word in sentence.split():\n",
    "                temp = ps.stem(word.lower())\n",
    "                temp_sent = temp_sent + temp + \" \"\n",
    "            print(temp_sent)\n",
    "            new_eng.at[count, 'text'] = temp_sent\n",
    "            new_eng.at[count, 'class'] = temp_df.iloc[count]['class']\n",
    "            count += 1\n",
    "        # print(new_eng)\n",
    "        return new_eng\n",
    "\n",
    "        stem(\"factionally\")\n",
    "\n",
    "    def clean_data(self, final_df):\n",
    "        # print(final_df)\n",
    "        eng_df = self.check_english(final_df)\n",
    "        # print(eng_df)\n",
    "        df_stem = self.Stemming(eng_df)\n",
    "        new_df = self.space(eng_df)\n",
    "        new_corpus_df = self.handle_negation(new_df)\n",
    "        remove_punc_df = self.remove_punc(new_corpus_df)\n",
    "        df_remove_stopWords = self.remove_stopwords(remove_punc_df)\n",
    "\n",
    "        new_corpus = self.text_concat(df_remove_stopWords)\n",
    "        li_new_corpus = new_corpus.split()\n",
    "        # print(remove_punc_df)\n",
    "        return li_new_corpus, df_remove_stopWords\n",
    "\n",
    "    def make_unique_li(self, li_cleanText):\n",
    "        unique_words_set = set(li_cleanText)\n",
    "        unique_word_li = list(unique_words_set)\n",
    "        return unique_word_li\n",
    "\n",
    "    def stemmed(self, li_cleanText):\n",
    "        count_stemed = 0\n",
    "        for word in li_cleanText:\n",
    "            if word[-1] == \"s\":\n",
    "                li_cleanText[count_stemed] = word[:-1]\n",
    "            elif word[-2:] == \"ed\":\n",
    "                li_cleanText[count_stemed] = word[:-2]\n",
    "            elif word[-3:] == \"ing\":\n",
    "                li_cleanText[count_stemed] = word[:-3]\n",
    "            count_stemed += 1\n",
    "        return li_cleanText\n",
    "\n",
    "    def Clean(self):\n",
    "        final_df, df = self.extract(pathData)\n",
    "        # corpus = clean.text_concat(final_df)\n",
    "        li_clean_text, df_clean = self.clean_data(final_df)\n",
    "        # print(\"ammar\")\n",
    "        uniqueWords = self.make_unique_li(li_clean_text)\n",
    "        return df_clean, uniqueWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76bef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8836135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f999c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "807e6ff8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_data_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3bacdb262427>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# COMMON WORD REMOVAL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m freq = pd.Series(\n\u001b[1;32m----> 3\u001b[1;33m     \" \".join(final_data_frame[\"text\"]).split()).value_counts()[:2]\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_data_frame' is not defined"
     ]
    }
   ],
   "source": [
    "# COMMON WORD REMOVAL\n",
    "freq = pd.Series(\n",
    "    \" \".join(final_data_frame[\"text\"]).split()).value_counts()[:2]\n",
    "print(freq)\n",
    "freq = list(freq.index)\n",
    "final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(\n",
    "    lambda x: \" \".join(x for x in x.split() if x not in freq)\n",
    ")\n",
    "print(\"removed comman words\")\n",
    "print(final_data_frame[\"text\"].head())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a92b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare = pd.Series(\n",
    "    \" \".join(final_data_frame[\"text\"]).split()).value_counts()[-10:]\n",
    "print(rare)\n",
    "rare = list(rare.index)\n",
    "final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(\n",
    "    lambda x: \" \".join(x for x in x.split() if x not in rare)\n",
    ")\n",
    "print(\"removed rare words\")\n",
    "print(final_data_frame[\"text\"].head())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45059655",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for text in final_data_frame[\"text\"]:\n",
    "    corpus.append(text)\n",
    "   # print(text)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE CLASS VALUES FROM YES/NO TO 0/1\n",
    "final_data_frame.rename(columns={\"class\": \"class_label\"}, inplace=True)\n",
    "Class_Label = {\"yes\": 1, \"no\": 0}\n",
    "final_data_frame.class_label = [\n",
    "    Class_Label[item] for item in final_data_frame.class_label\n",
    "]\n",
    "final_data_frame.rename(columns={\"class_label\": \"class\"}, inplace=True)\n",
    "print(\"rename values of class column\")\n",
    "print(final_data_frame.head())\n",
    "print()\n",
    "# ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120bc09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "# Transforms text into a sparse matrix of n-gram counts.\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorized_data = count_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221fbb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF\n",
    "# Performs the TF-IDF transformation from a provided matrix of counts.\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorized_data = tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a65b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose document vector\n",
    "vectorized_data = tfidf_vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2057e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLITING THE DATA\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    vectorized_data, final_data_frame[\"class\"], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90710ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cad8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(probability=True, C=1.0, kernel=\"linear\", degree=3, gamma=\"auto\")\n",
    "SVM.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd815b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d5871",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7545902",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5987116",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = SVM.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ed2f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015cf4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=['i will buy iphone','iphone is not all that great they are all the same']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ca768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF\n",
    "# Performs the TF-IDF transformation from a provided matrix of counts.\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorized_data1 = tfidf_vectorizer.fit_transform(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_vectorized_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5bce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose document vector\n",
    "vectorized_data1 = tfidf_vectorized_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9357d40d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee01888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22168939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b540d7d656740333ba454541d09a5f3ddb0bbc0345a85d382fa060d42a00adee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
