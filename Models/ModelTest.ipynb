{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d2af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f1811b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21178abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c3f3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a83aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da121eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(path):\n",
    "    fd = open(path, encoding=\"utf-8\", errors=\"replace\")\n",
    "    df = pd.read_csv(fd)\n",
    "    defined = df[\"class\"] != (\"undefined\")\n",
    "    # #output dataframe without undeined\n",
    "    df2 = df[defined]\n",
    "    defined1 = df2[\"class\"] != \"Undefined\"\n",
    "    df4 = df2[defined1]\n",
    "    # replace no PI with no\n",
    "    df3 = df4.replace(\"No PI\", \"no\")\n",
    "    # replace PI with yes\n",
    "    final = df3.replace(\"PI\", \"yes\")\n",
    "    replace_yes = final.replace(\"Yes\", \"yes\")\n",
    "    final_df = replace_yes.replace(\"No\", \"no\")\n",
    "    return final_df, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ce7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_negation(final_df):\n",
    "    out_df = pd.DataFrame()\n",
    "    count_tweet = 0\n",
    "    for text in final_df['text']:\n",
    "        temp_text = \"\"\n",
    "        li_text = text.split()\n",
    "        for word in li_text:\n",
    "            count = 0\n",
    "            lower_word = word.lower()\n",
    "            if lower_word == \"didn't\" or lower_word == \"not\" or lower_word == \"no\" or lower_word == \"never\"\\\n",
    "                    or lower_word == \"don't\":\n",
    "                temp = count + 1\n",
    "                temp_text = temp_text + word + \" \"\n",
    "                for i in range(temp, len(li_text)):\n",
    "                    if li_text[i] in [\",\", \"?\", \"!\", \".\"]:\n",
    "                        temp_text = \" \"+temp_text + li_text[i] + \" \"\n",
    "                        break\n",
    "                    else:\n",
    "                        temp_text = temp_text + \"NOT_\" + li_text[i]+\" \"\n",
    "\n",
    "            else:\n",
    "                temp_text = temp_text + word + \" \"\n",
    "        # print(temp_text)\n",
    "        out_df.at[count_tweet, 'text'] = temp_text\n",
    "        out_df.at[count_tweet, 'class'] = final_df.iloc[count_tweet]['class']\n",
    "        count_tweet += 1\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e18e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def space(final_df):\n",
    "    new_df = pd.DataFrame()\n",
    "    count_tweets = 0\n",
    "    for text in final_df['text']:\n",
    "        temp = \"\"\n",
    "        for char in text:\n",
    "            if char in [\",\", \".\", \"!\", \"?\", \":\", \";\"]:\n",
    "                temp = temp + ' ' + char\n",
    "\n",
    "            else:\n",
    "                temp = temp + char\n",
    "        # print(temp)\n",
    "        new_df.at[count_tweets, 'text'] = temp\n",
    "        new_df.at[count_tweets, 'class'] = final_df.iloc[count_tweets]['class']\n",
    "        count_tweets += 1\n",
    "    # print(\"new_df\")\n",
    "    # print(new_df)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1df3c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(model, X, y):\n",
    "    # print(X.shape)\n",
    "    # print(y.shape)\n",
    "    pred_proba = model.predict_proba(X)[:, 1]\n",
    "    pred = model.predict(X)\n",
    "    auc = roc_auc_score(y, pred_proba)\n",
    "    acc = accuracy_score(y, pred)\n",
    "    f1 = f1_score(y, pred)\n",
    "    prec = precision_score(y, pred)\n",
    "    rec = recall_score(y, pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    TrueNeg = tn / (tn + fp)\n",
    "    result = {\n",
    "        \"auc\": auc,\n",
    "        \"f1\": f1,\n",
    "        \"acc\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn,\n",
    "        \"TP\": tp,\n",
    "        \"True Negative rate\": TrueNeg,\n",
    "    }\n",
    "    return result, pred, pred_proba, acc, TrueNeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c32004ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(final_data_frame):\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Data cleaning step wise\n",
    "    # -----------------------------------------------------------------------\n",
    "    # 1. LOWERCASE\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(\n",
    "        lambda x: \" \".join(x.lower() for x in x.split())\n",
    "    )\n",
    "    # NEGATION HANDLING\n",
    "    final_data_frame = space(final_data_frame)\n",
    "    final_data_frame = handle_negation(final_data_frame)\n",
    "    # 2. REMOVE PUNC\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].str.replace(\"[^\\w\\s]\", \"\",regex=True)\n",
    "    # 3. STOPWORDS REMOVAL\n",
    "    stop = stopwords.words(\"english\")\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    # 4. COMMON WORD REMOVAL\n",
    "    freq = pd.Series(\" \".join(final_data_frame[\"text\"]).split()).value_counts()[:2]\n",
    "    freq = list(freq.index)\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "    # 5. RARE WORDS REMOVAL\n",
    "    rare = pd.Series(\" \".join(final_data_frame[\"text\"]).split()).value_counts()[-10:]\n",
    "    rare = list(rare.index)\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\n",
    "    # 6. SPELLING CORRECTION\n",
    "    final_data_frame[\"text\"]= final_data_frame[\"text\"][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "    # 7. STEMMING\n",
    "    #st = PorterStemmer()\n",
    "   # final_data_frame[\"text\"].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "    # 8. LEMMATIZATION\n",
    "    \n",
    "    # -----------------------------------------------------------------------\n",
    "    # BUILDING THE CORPUS\n",
    "    #-----------------------------------------------------------------------\n",
    "    corpus = []\n",
    "    for text in final_data_frame[\"text\"]:\n",
    "        corpus.append(text)\n",
    "        \n",
    "    # -----------------------------------------------------------------------\n",
    "    # CHANGE CLASS VALUES FROM YES/NO TO 0/1\n",
    "    # -----------------------------------------------------------------------\n",
    "    final_data_frame.rename(columns={\"class\": \"class_label\"}, inplace=True)\n",
    "    Class_Label = {\"yes\": 1, \"no\": 0}\n",
    "    final_data_frame.class_label = [\n",
    "        Class_Label[item] for item in final_data_frame.class_label\n",
    "    ]\n",
    "    final_data_frame.rename(columns={\"class_label\": \"class\"}, inplace=True)\n",
    "    return final_data_frame, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8734d5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text class\n",
      "0  !!!My ManagERs Told @ME He is GoInG tO  gEts  ...   yes\n"
     ]
    }
   ],
   "source": [
    "final_data_frame,df=extract(\"C:/Users/su/Desktop/Major_Project_work/data/newdata.csv\")\n",
    "print(final_data_frame)\n",
    "# final_data_frame,corpus=data_preprocessing(final_data_frame)\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "599958fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text class\n",
      "0  !!!My ManagERs Told @ME He is GoInG tO  gEts  ...   yes\n",
      "                                                text class\n",
      "0  !!!my managers told @me he is going to gets th...   yes\n"
     ]
    }
   ],
   "source": [
    " # 1. LOWERCASE\n",
    "print(final_data_frame)\n",
    "final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(\n",
    "        lambda x: \" \".join(x.lower() for x in x.split())\n",
    "    )\n",
    "print(final_data_frame)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f48a65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e18d9f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text class\n",
      "0  !!!my managers told @me he is going to gets th...   yes\n",
      "                                                text class\n",
      "0  my managers told me he is going to gets the ip...   yes\n"
     ]
    }
   ],
   "source": [
    "# 2. REMOVE PUNC\n",
    "print(final_data_frame) \n",
    "final_data_frame[\"text\"] = final_data_frame[\"text\"].str.replace(\"[^\\w\\s]\", \"\",regex=True)\n",
    "print(final_data_frame)    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6597107a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text class\n",
      "0  my managers told me he is going to gets the ip...   yes\n",
      "                                             text class\n",
      "0  managers told going gets iphone x entertaiment   yes\n"
     ]
    }
   ],
   "source": [
    " # 3. STOPWORDS REMOVAl\n",
    "print(final_data_frame) \n",
    "stop = stopwords.words(\"english\")\n",
    "final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "print(final_data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b078065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4. COMMON WORD REMOVAL\n",
    "# freq = pd.Series(\" \".join(final_data_frame[\"text\"]).split()).value_counts()[:5]\n",
    "# freq = list(freq.index)\n",
    "# final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "# print(freq) \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8360539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # 5. RARE WORDS REMOVAL\n",
    "# rare = pd.Series(\" \".join(final_data_frame[\"text\"]).split()).value_counts()[-10:]\n",
    "# rare = list(rare.index)\n",
    "# final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\n",
    "#  print(rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da361d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6318c435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             text class\n",
      "0  managers told going gets iphone x entertaiment   yes\n",
      "                                             text class\n",
      "0  managers told going gets phone x entertainment   yes\n"
     ]
    }
   ],
   "source": [
    "# 6. SPELLING CORRECTION\n",
    "print(final_data_frame) \n",
    "final_data_frame[\"text\"] =final_data_frame[\"text\"][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "print(final_data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5fc0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # 7. STEMMING\n",
    "# print(final_data_frame) \n",
    "# st = PorterStemmer()\n",
    "# final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join([st.stem(word) for word in x.split(\" \")]))\n",
    "# print(final_data_frame)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567a39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb3cfe0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             text class\n",
      "0  managers told going gets phone x entertainment   yes\n",
      "                                           text class\n",
      "0  manager told going get phone x entertainment   yes\n"
     ]
    }
   ],
   "source": [
    "# 8. LEMMATIZATION\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "print(final_data_frame)\n",
    "final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join(\n",
    "    [wordnet_lemmatizer.lemmatize(word) for word in x.split(\" \")]))\n",
    "print(final_data_frame)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f9fcbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manager told going get phone x entertainment']\n"
     ]
    }
   ],
   "source": [
    " # -----------------------------------------------------------------------\n",
    "    # BUILDING THE CORPUS\n",
    "    #-----------------------------------------------------------------------\n",
    "corpus = []\n",
    "for text in final_data_frame[\"text\"]:\n",
    "    corpus.append(text)\n",
    "    \n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80298db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eca7486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_results(pathData_train, pathData_test, doc_vector, model):\n",
    "    output_data_frame = pd.DataFrame()\n",
    "    train_data, train_data_undefined = extract(pathData_train)\n",
    "    test_data, test_data_undefined = extract(pathData_test)\n",
    "    output_data_frame['tweets'] = test_data['text']\n",
    "    train_data, train_corpus = data_preprocessing(train_data)\n",
    "    test_data, test_corpus = data_preprocessing(test_data)\n",
    "    output_data_frame['processed tweets'] = test_data['text']\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Select Document vector\n",
    "    # -----------------------------------------------------------------------\n",
    "    if doc_vector == \"TF\":\n",
    "        count_vectorizer = CountVectorizer()\n",
    "        count_vectorized_data_train = count_vectorizer.fit_transform(\n",
    "            train_corpus)\n",
    "        vectorized_data_train = count_vectorized_data_train\n",
    "        # count_vectorized_data_test = count_vectorizer.fit_transform(test_corpus)\n",
    "        count_vectorized_data_test = count_vectorizer.transform(test_corpus)\n",
    "        vectorized_data_test = count_vectorized_data_test\n",
    "    elif doc_vector == \"TF-IDF\":\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_vectorized_data_train = tfidf_vectorizer.fit_transform(\n",
    "            train_corpus)\n",
    "        vectorized_data_train = tfidf_vectorized_data_train\n",
    "        tfidf_vectorized_data_test = tfidf_vectorizer.transform(test_corpus)\n",
    "        vectorized_data_test = tfidf_vectorized_data_test\n",
    "    # -----------------------------------------------------------------------\n",
    "    # training/testing\n",
    "    # -----------------------------------------------------------------------\n",
    "    X_train = vectorized_data_train\n",
    "    X_test = vectorized_data_test\n",
    "    Y_train = train_data[\"class\"].values\n",
    "    Y_test = test_data[\"class\"].values\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Select model to train and display stats\n",
    "    # -----------------------------------------------------------------------\n",
    "    if model == \"SVM\":\n",
    "        SVM = svm.SVC(probability=True, C=1.0,\n",
    "                      kernel=\"linear\", degree=3, gamma=\"auto\")\n",
    "        SVM.fit(X_train, Y_train)\n",
    "\n",
    "        stats, pred, pred_proba, acc, trueNeg = report_results(\n",
    "            SVM, X_test, Y_test)\n",
    "\n",
    "    elif model == \"Naive Bayes\":\n",
    "        Naive = naive_bayes.MultinomialNB()\n",
    "        Naive.fit(X_train, Y_train)\n",
    "\n",
    "        stats, pred, pred_proba, acc, trueNeg = report_results(\n",
    "            Naive, X_test, Y_test)\n",
    "\n",
    "    elif model == \"Logistic Regression\":\n",
    "        logisticReg = linear_model.LogisticRegression(C=1.0)\n",
    "        logisticReg.fit(X_train, Y_train)\n",
    "\n",
    "        stats, pred, pred_proba, acc, trueNeg = report_results(\n",
    "            logisticReg, X_test, Y_test)\n",
    "\n",
    "    elif model == \"Decision Tree\":\n",
    "        DTC = DecisionTreeClassifier(min_samples_split=7, random_state=252)\n",
    "        DTC.fit(X_train, Y_train)\n",
    "\n",
    "        stats, pred, pred_proba, acc, trueNeg = report_results(\n",
    "            DTC, X_test, Y_test)\n",
    "\n",
    "    elif model == \"Neural Network\":\n",
    "        neural_network = MLPClassifier(\n",
    "            solver=\"lbfgs\", alpha=1e-5, hidden_layer_sizes=(20, 10, 10, 5), random_state=1\n",
    "        )\n",
    "        neural_network.fit(X_train, Y_train)\n",
    "\n",
    "        stats, pred, pred_proba, acc, trueNeg = report_results(\n",
    "            neural_network, X_test, Y_test)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    test_data['Predicted Class'] = pred.tolist()\n",
    "    output_data_frame['true class'] = test_data['class']\n",
    "    output_data_frame['prediced class'] = pred.tolist()\n",
    "    output_data_frame['score assigned by model'] = pred_proba.tolist()\n",
    "    test_data['score'] = pred_proba.tolist()\n",
    "    # print(test_data['score'])\n",
    "    test_data['class'].replace(0, \"no\", inplace=True)\n",
    "    test_data['class'].replace(1, \"yes\", inplace=True)\n",
    "    test_data['Predicted Class'].replace(0, \"no\", inplace=True)\n",
    "    test_data['Predicted Class'].replace(1, \"yes\", inplace=True)\n",
    "    #print(test_data['Predicted Class'])\n",
    "\n",
    "    output_data_frame['true class'].replace(0, \"no\", inplace=True)\n",
    "    output_data_frame['true class'].replace(1, \"yes\", inplace=True)\n",
    "    output_data_frame['prediced class'].replace(0, \"no\", inplace=True)\n",
    "    output_data_frame['prediced class'].replace(1, \"yes\", inplace=True)\n",
    "\n",
    "    output_data_frame['model'] = model\n",
    "    output_data_frame['docVec'] = doc_vector\n",
    "    output_data_frame['acc'] = acc\n",
    "    output_data_frame['trueNeg'] = trueNeg\n",
    "\n",
    "    return stats, test_data, output_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a1271a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_data_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-bf16f6854adb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_data_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'output_data_frame' is not defined"
     ]
    }
   ],
   "source": [
    "print(output_data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37b402e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-768a26083dc9>:22: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  rare = pd.Series(\" \".join(final_data_frame[\"text\"]).split()).value_counts()[-10:]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-b3961dadd32a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_data_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_to_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/su/Desktop/Major_Project_work/data/training.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"C:/Users/su/Desktop/Major_Project_work/data/Book1.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"TF-IDF\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"SVM\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-e239ce60453e>\u001b[0m in \u001b[0;36moutput_to_results\u001b[1;34m(pathData_train, pathData_test, doc_vector, model)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdoc_vector\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"TF-IDF\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtfidf_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         tfidf_vectorized_data_train = tfidf_vectorizer.fit_transform(\n\u001b[0m\u001b[0;32m     23\u001b[0m             train_corpus)\n\u001b[0;32m     24\u001b[0m         \u001b[0mvectorized_data_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorized_data_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \"\"\"\n\u001b[0;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1850\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1115\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1116\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[0;32m    218\u001b[0m                              \"unicode string.\")\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "stats, test_data, output_data_frame = output_to_results(\"C:/Users/su/Desktop/Major_Project_work/data/training.csv\",\"C:/Users/su/Desktop/Major_Project_work/data/Book1.csv\", \"TF-IDF\", \"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = \"C:/Users/su/Desktop/\"\n",
    "model_selection = \"decision tree TF-idf neghandling lemmitization1\"\n",
    "ext = \".csv\"\n",
    "file_name = path_to_save + model_selection + ext\n",
    "\n",
    "export = output_data_frame.to_csv(r''+file_name, index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8acf1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[\"i want samsung\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44e3e3c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-6781dc195a9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfidf_vectorized_data_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_vectorized_data_test = tfidf_vectorizer.transform(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6adb52d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
