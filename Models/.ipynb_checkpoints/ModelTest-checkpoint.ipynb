{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d2af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "4f1811b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21178abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2c3f3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a83aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da121eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(path):\n",
    "    fd = open(path, encoding=\"utf-8\", errors=\"replace\")\n",
    "    df = pd.read_csv(fd)\n",
    "    defined = df[\"class\"] != (\"undefined\")\n",
    "    # #output dataframe without undeined\n",
    "    df2 = df[defined]\n",
    "    defined1 = df2[\"class\"] != \"Undefined\"\n",
    "    df4 = df2[defined1]\n",
    "    # replace no PI with no\n",
    "    df3 = df4.replace(\"No PI\", \"no\")\n",
    "    # replace PI with yes\n",
    "    final = df3.replace(\"PI\", \"yes\")\n",
    "    replace_yes = final.replace(\"Yes\", \"yes\")\n",
    "    final_df = replace_yes.replace(\"No\", \"no\")\n",
    "    return final_df, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16ce7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_negation(final_df):\n",
    "    out_df = pd.DataFrame()\n",
    "    count_tweet = 0\n",
    "    for text in final_df['text']:\n",
    "        temp_text = \"\"\n",
    "        li_text = text.split()\n",
    "        for word in li_text:\n",
    "            count = 0\n",
    "            lower_word = word.lower()\n",
    "            if lower_word == \"didn't\" or lower_word == \"not\" or lower_word == \"no\" or lower_word == \"never\"\\\n",
    "                    or lower_word == \"don't\":\n",
    "                temp = count + 1\n",
    "                temp_text = temp_text + word + \" \"\n",
    "                for i in range(temp, len(li_text)):\n",
    "                    if li_text[i] in [\",\", \"?\", \"!\", \".\"]:\n",
    "                        temp_text = \" \"+temp_text + li_text[i] + \" \"\n",
    "                        break\n",
    "                    else:\n",
    "                        temp_text = temp_text + \"NOT_\" + li_text[i]+\" \"\n",
    "\n",
    "            else:\n",
    "                temp_text = temp_text + word + \" \"\n",
    "        # print(temp_text)\n",
    "        out_df.at[count_tweet, 'text'] = temp_text\n",
    "        out_df.at[count_tweet, 'class'] = final_df.iloc[count_tweet]['class']\n",
    "        count_tweet += 1\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e18e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def space(final_df):\n",
    "    new_df = pd.DataFrame()\n",
    "    count_tweets = 0\n",
    "    for text in final_df['text']:\n",
    "        temp = \"\"\n",
    "        for char in text:\n",
    "            if char in [\",\", \".\", \"!\", \"?\", \":\", \";\"]:\n",
    "                temp = temp + ' ' + char\n",
    "\n",
    "            else:\n",
    "                temp = temp + char\n",
    "        # print(temp)\n",
    "        new_df.at[count_tweets, 'text'] = temp\n",
    "        new_df.at[count_tweets, 'class'] = final_df.iloc[count_tweets]['class']\n",
    "        count_tweets += 1\n",
    "    # print(\"new_df\")\n",
    "    # print(new_df)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1df3c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(model, X, y):\n",
    "    # print(X.shape)\n",
    "    # print(y.shape)\n",
    "    pred_proba = model.predict_proba(X)[:, 1]\n",
    "    pred = model.predict(X)\n",
    "    auc = roc_auc_score(y, pred_proba)\n",
    "    acc = accuracy_score(y, pred)\n",
    "    f1 = f1_score(y, pred)\n",
    "    prec = precision_score(y, pred)\n",
    "    rec = recall_score(y, pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    TrueNeg = tn / (tn + fp)\n",
    "    result = {\n",
    "        \"auc\": auc,\n",
    "        \"f1\": f1,\n",
    "        \"acc\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn,\n",
    "        \"TP\": tp,\n",
    "        \"True Negative rate\": TrueNeg,\n",
    "    }\n",
    "    return result, pred, pred_proba, acc, TrueNeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c32004ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(final_data_frame):\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Data cleaning step wise\n",
    "    # -----------------------------------------------------------------------\n",
    "    # 1. LOWERCASE\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(\n",
    "        lambda x: \" \".join(x.lower() for x in x.split())\n",
    "    )\n",
    "    # NEGATION HANDLING\n",
    "    final_data_frame = space(final_data_frame)\n",
    "    final_data_frame = handle_negation(final_data_frame)\n",
    "    # 2. REMOVE PUNC\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].str.replace(\"[^\\w\\s]\", \"\",regex=True)\n",
    "    # 3. STOPWORDS REMOVAL\n",
    "    stop = stopwords.words(\"english\")\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    # 4. COMMON WORD REMOVAL\n",
    "    freq = pd.Series(\" \".join(final_data_frame[\"text\"]).split()).value_counts()[:2]\n",
    "    freq = list(freq.index)\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "    # 5. RARE WORDS REMOVAL\n",
    "    rare = pd.Series(\" \".join(final_data_frame[\"text\"]).split()).value_counts()[-10:]\n",
    "    rare = list(rare.index)\n",
    "    final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\n",
    "    # 6. SPELLING CORRECTION\n",
    "    final_data_frame[\"text\"]= final_data_frame[\"text\"][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "    # 7. STEMMING\n",
    "    st = PorterStemmer()\n",
    "    final_data_frame[\"text\"]= final_data_frame[\"text\"].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "    # 8. LEMMATIZATION\n",
    "    \n",
    "    # -----------------------------------------------------------------------\n",
    "    # BUILDING THE CORPUS\n",
    "    #-----------------------------------------------------------------------\n",
    "    corpus = []\n",
    "    for text in final_data_frame[\"text\"]:\n",
    "        corpus.append(text)\n",
    "        \n",
    "    # -----------------------------------------------------------------------\n",
    "    # CHANGE CLASS VALUES FROM YES/NO TO 0/1\n",
    "    # -----------------------------------------------------------------------\n",
    "    final_data_frame.rename(columns={\"class\": \"class_label\"}, inplace=True)\n",
    "    Class_Label = {\"yes\": 1, \"no\": 0}\n",
    "    final_data_frame.class_label = [\n",
    "        Class_Label[item] for item in final_data_frame.class_label\n",
    "    ]\n",
    "    final_data_frame.rename(columns={\"class_label\": \"class\"}, inplace=True)\n",
    "    return final_data_frame, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "8734d5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class                                               text Unnamed: 2\n",
      "0      yes  What's the difference between the  iPhone X  a...        NaN\n",
      "1       no  @ siracusa  just listened to Hypercritical #16...        NaN\n",
      "2      yes  Same.  IPhone X  won't be bad   https:// twitt...        NaN\n",
      "4       no  “  iphone x  for £150” do i have dickhead writ...        NaN\n",
      "5      yes       This  iPhone X  still very sexy a year later        NaN\n",
      "...    ...                                                ...        ...\n",
      "1675    no  Ew  iPhone  7’s camera quality looks so trash ...        NaN\n",
      "1677    no  @ Apple  hey why does your brand new  expensiv...        NaN\n",
      "1678    no  Unpopular opinion: I  hate  the  iPhone   X  m...        NaN\n",
      "1681   yes               i want the  iphone   x  so  bad  :’(        NaN\n",
      "1682    no  can anyone explain to me why the incredibly  e...        NaN\n",
      "\n",
      "[903 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "final_data_frame,df=extract(\"C:/Users/su/Desktop/Major_Project_work/data/training.csv\")\n",
    "print(final_data_frame)\n",
    "# final_data_frame,corpus=data_preprocessing(final_data_frame)\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "599958fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class                                               text Unnamed: 2\n",
      "0      yes  What's the difference between the  iPhone X  a...        NaN\n",
      "1       no  @ siracusa  just listened to Hypercritical #16...        NaN\n",
      "2      yes  Same.  IPhone X  won't be bad   https:// twitt...        NaN\n",
      "4       no  “  iphone x  for £150” do i have dickhead writ...        NaN\n",
      "5      yes       This  iPhone X  still very sexy a year later        NaN\n",
      "...    ...                                                ...        ...\n",
      "1675    no  Ew  iPhone  7’s camera quality looks so trash ...        NaN\n",
      "1677    no  @ Apple  hey why does your brand new  expensiv...        NaN\n",
      "1678    no  Unpopular opinion: I  hate  the  iPhone   X  m...        NaN\n",
      "1681   yes               i want the  iphone   x  so  bad  :’(        NaN\n",
      "1682    no  can anyone explain to me why the incredibly  e...        NaN\n",
      "\n",
      "[903 rows x 3 columns]\n",
      "     class                                               text Unnamed: 2\n",
      "0      yes  what's the difference between the iphone x and...        NaN\n",
      "1       no  @ siracusa just listened to hypercritical #16,...        NaN\n",
      "2      yes  same. iphone x won't be bad https:// twitter.c...        NaN\n",
      "4       no  “ iphone x for £150” do i have dickhead writte...        NaN\n",
      "5      yes         this iphone x still very sexy a year later        NaN\n",
      "...    ...                                                ...        ...\n",
      "1675    no  ew iphone 7’s camera quality looks so trash ne...        NaN\n",
      "1677    no  @ apple hey why does your brand new expensive ...        NaN\n",
      "1678    no  unpopular opinion: i hate the iphone x models ...        NaN\n",
      "1681   yes                     i want the iphone x so bad :’(        NaN\n",
      "1682    no  can anyone explain to me why the incredibly ex...        NaN\n",
      "\n",
      "[903 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    " # 1. LOWERCASE\n",
    "print(final_data_frame)\n",
    "final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(\n",
    "        lambda x: \" \".join(x.lower() for x in x.split())\n",
    "    )\n",
    "print(final_data_frame)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c3989f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "cf9e5512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class                                               text Unnamed: 2\n",
      "0      yes  what's the difference between the iphone x and...        NaN\n",
      "1       no  @ siracusa just listened to hypercritical #16,...        NaN\n",
      "2      yes  same. iphone x won't be bad https:// twitter.c...        NaN\n",
      "4       no  “ iphone x for £150” do i have dickhead writte...        NaN\n",
      "5      yes         this iphone x still very sexy a year later        NaN\n",
      "...    ...                                                ...        ...\n",
      "1675    no  ew iphone 7’s camera quality looks so trash ne...        NaN\n",
      "1677    no  @ apple hey why does your brand new expensive ...        NaN\n",
      "1678    no  unpopular opinion: i hate the iphone x models ...        NaN\n",
      "1681   yes                     i want the iphone x so bad :’(        NaN\n",
      "1682    no  can anyone explain to me why the incredibly ex...        NaN\n",
      "\n",
      "[903 rows x 3 columns]\n",
      "     class                                               text Unnamed: 2\n",
      "0      yes  whats the difference between the iphone x and ...        NaN\n",
      "1       no   siracusa just listened to hypercritical 16 wh...        NaN\n",
      "2      yes  same iphone x wont be bad https twittercomafua...        NaN\n",
      "4       no   iphone x for 150 do i have dickhead written a...        NaN\n",
      "5      yes         this iphone x still very sexy a year later        NaN\n",
      "...    ...                                                ...        ...\n",
      "1675    no  ew iphone 7s camera quality looks so trash nex...        NaN\n",
      "1677    no   apple hey why does your brand new expensive i...        NaN\n",
      "1678    no  unpopular opinion i hate the iphone x models y...        NaN\n",
      "1681   yes                        i want the iphone x so bad         NaN\n",
      "1682    no  can anyone explain to me why the incredibly ex...        NaN\n",
      "\n",
      "[903 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# 2. REMOVE PUNC\n",
    "print(final_data_frame) \n",
    "final_data_frame[\"text\"] = final_data_frame[\"text\"].str.replace(\"[^\\w\\s]\", \"\",regex=True)\n",
    "print(final_data_frame)    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c0531fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class                                               text Unnamed: 2\n",
      "0      yes  whats the difference between the iphone x and ...        NaN\n",
      "1       no   siracusa just listened to hypercritical 16 wh...        NaN\n",
      "2      yes  same iphone x wont be bad https twittercomafua...        NaN\n",
      "4       no   iphone x for 150 do i have dickhead written a...        NaN\n",
      "5      yes         this iphone x still very sexy a year later        NaN\n",
      "...    ...                                                ...        ...\n",
      "1675    no  ew iphone 7s camera quality looks so trash nex...        NaN\n",
      "1677    no   apple hey why does your brand new expensive i...        NaN\n",
      "1678    no  unpopular opinion i hate the iphone x models y...        NaN\n",
      "1681   yes                        i want the iphone x so bad         NaN\n",
      "1682    no  can anyone explain to me why the incredibly ex...        NaN\n",
      "\n",
      "[903 rows x 3 columns]\n",
      "     class                                               text Unnamed: 2\n",
      "0      yes           whats difference iphone x iphone x x max        NaN\n",
      "1       no  siracusa listened hypercritical 16 whole episo...        NaN\n",
      "2      yes  iphone x wont bad https twittercomafuaasamstat...        NaN\n",
      "4       no                      iphone x 150 dickhead written        NaN\n",
      "5      yes                     iphone x still sexy year later        NaN\n",
      "...    ...                                                ...        ...\n",
      "1675    no  ew iphone 7s camera quality looks trash next x...        NaN\n",
      "1677    no  apple hey brand new expensive iphone x suck ca...        NaN\n",
      "1678    no  unpopular opinion hate iphone x models yet won...        NaN\n",
      "1681   yes                                  want iphone x bad        NaN\n",
      "1682    no  anyone explain incredibly expensive iphone x r...        NaN\n",
      "\n",
      "[903 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    " # 3. STOPWORDS REMOVAl\n",
    "print(final_data_frame) \n",
    "stop = stopwords.words(\"english\")\n",
    "final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "print(final_data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "638e4edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iphone', 'x']\n"
     ]
    }
   ],
   "source": [
    "# 4. COMMON WORD REMOVAL\n",
    "freq = pd.Series(\" \".join(final_data_frame[\"text\"]).split()).value_counts()[:2]\n",
    "freq = list(freq.index)\n",
    "final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "print(freq) \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "251ac8bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 5: expected str instance, float found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-281-84b5ebde407c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 5. RARE WORDS REMOVAL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrare\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_data_frame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mrare\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrare\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfinal_data_frame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_data_frame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 5: expected str instance, float found"
     ]
    }
   ],
   "source": [
    " # 5. RARE WORDS REMOVAL\n",
    "rare = pd.Series(\" \".join(final_data_frame[\"text\"]).split()).value_counts()[-10:]\n",
    "rare = list(rare.index)\n",
    "final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "641814d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reported', 'youtubee6rmnflofe8a', 'input', 'greatest', 'ending', 'onlyyyy', 'authority', 'replaced', 'randomly', 'onlyinafrica']\n"
     ]
    }
   ],
   "source": [
    "print(rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "4c33e9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class                                               text Unnamed: 2\n",
      "0      yes                               whats difference max        NaN\n",
      "1       no  siracusa listened hypercritical 16 whole episo...        NaN\n",
      "2      yes  wont bad https twittercomafuaasamstatu s106824...        NaN\n",
      "4       no                               150 dickhead written        NaN\n",
      "5      yes                                   still year later        NaN\n",
      "...    ...                                                ...        ...\n",
      "1675    no    ew 7s camera quality looks trash next hate poor        NaN\n",
      "1677    no  apple hey brand new expensive suck cant even p...        NaN\n",
      "1678    no  unpopular opinion hate models yet wont get ano...        NaN\n",
      "1681   yes                                           want bad        NaN\n",
      "1682    no  anyone explain incredibly expensive slide perf...        NaN\n",
      "\n",
      "[903 rows x 3 columns]\n",
      "     class                                               text Unnamed: 2\n",
      "0      yes                                what difference max        NaN\n",
      "1       no  siracusa listened hypercritical 16 whole episo...        NaN\n",
      "2      yes  wont bad http twittercomafuaasamstatu s1068249...        NaN\n",
      "4       no                               150 dickhead written        NaN\n",
      "5      yes                                   still year later        NaN\n",
      "...    ...                                                ...        ...\n",
      "1675    no                                                NaN        NaN\n",
      "1677    no                                                NaN        NaN\n",
      "1678    no                                                NaN        NaN\n",
      "1681   yes                                                NaN        NaN\n",
      "1682    no                                                NaN        NaN\n",
      "\n",
      "[903 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# 6. SPELLING CORRECTION\n",
    "print(final_data_frame) \n",
    "final_data_frame[\"text\"] =final_data_frame[\"text\"][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "print(final_data_frame)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "19f9fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # 7. STEMMING\n",
    "# print(final_data_frame) \n",
    "# st = PorterStemmer()\n",
    "# final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join([st.stem(word) for word in x.split(\" \")]))\n",
    "# print(final_data_frame)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8179eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c01bbc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             text class\n",
      "0  managers told going gets phone x entertainment   yes\n",
      "                                           text class\n",
      "0  manager told going get phone x entertainment   yes\n"
     ]
    }
   ],
   "source": [
    "# 8. LEMMATIZATION\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "print(final_data_frame)\n",
    "final_data_frame[\"text\"] = final_data_frame[\"text\"].apply(lambda x: \" \".join([wordnet_lemmatizer.lemmatize(word) for word in x.split(\" \")]))\n",
    "print(final_data_frame)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "93702f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manager told going get phone x entertainment httpstwittercomnevadaaa___st atus931251491138129920']\n"
     ]
    }
   ],
   "source": [
    " # -----------------------------------------------------------------------\n",
    "    # BUILDING THE CORPUS\n",
    "    #-----------------------------------------------------------------------\n",
    "corpus = []\n",
    "for text in final_data_frame[\"text\"]:\n",
    "    corpus.append(text)\n",
    "    \n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373fc43f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eca7486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_results(pathData_train, pathData_test, doc_vector, model):\n",
    "    output_data_frame = pd.DataFrame()\n",
    "    train_data, train_data_undefined = extract(pathData_train)\n",
    "    test_data, test_data_undefined = extract(pathData_test)\n",
    "    output_data_frame['tweets'] = test_data['text']\n",
    "    train_data, train_corpus = data_preprocessing(train_data)\n",
    "    test_data, test_corpus = data_preprocessing(test_data)\n",
    "    output_data_frame['processed tweets'] = test_data['text']\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Select Document vector\n",
    "    # -----------------------------------------------------------------------\n",
    "    if doc_vector == \"TF\":\n",
    "        count_vectorizer = CountVectorizer()\n",
    "        count_vectorized_data_train = count_vectorizer.fit_transform(\n",
    "            train_corpus)\n",
    "        vectorized_data_train = count_vectorized_data_train\n",
    "        # count_vectorized_data_test = count_vectorizer.fit_transform(test_corpus)\n",
    "        count_vectorized_data_test = count_vectorizer.transform(test_corpus)\n",
    "        vectorized_data_test = count_vectorized_data_test\n",
    "    elif doc_vector == \"TF-IDF\":\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_vectorized_data_train = tfidf_vectorizer.fit_transform(\n",
    "            train_corpus)\n",
    "        vectorized_data_train = tfidf_vectorized_data_train\n",
    "        tfidf_vectorized_data_test = tfidf_vectorizer.transform(test_corpus)\n",
    "        vectorized_data_test = tfidf_vectorized_data_test\n",
    "    # -----------------------------------------------------------------------\n",
    "    # training/testing\n",
    "    # -----------------------------------------------------------------------\n",
    "    X_train = vectorized_data_train\n",
    "    X_test = vectorized_data_test\n",
    "    Y_train = train_data[\"class\"].values\n",
    "    Y_test = test_data[\"class\"].values\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Select model to train and display stats\n",
    "    # -----------------------------------------------------------------------\n",
    "    if model == \"SVM\":\n",
    "        SVM = svm.SVC(probability=True, C=1.0,\n",
    "                      kernel=\"linear\", degree=3, gamma=\"auto\")\n",
    "        SVM.fit(X_train, Y_train)\n",
    "\n",
    "        stats, pred, pred_proba, acc, trueNeg = report_results(\n",
    "            SVM, X_test, Y_test)\n",
    "\n",
    "    elif model == \"Naive Bayes\":\n",
    "        Naive = naive_bayes.MultinomialNB()\n",
    "        Naive.fit(X_train, Y_train)\n",
    "\n",
    "        stats, pred, pred_proba, acc, trueNeg = report_results(\n",
    "            Naive, X_test, Y_test)\n",
    "\n",
    "    elif model == \"Logistic Regression\":\n",
    "        logisticReg = linear_model.LogisticRegression(C=1.0)\n",
    "        logisticReg.fit(X_train, Y_train)\n",
    "\n",
    "        stats, pred, pred_proba, acc, trueNeg = report_results(\n",
    "            logisticReg, X_test, Y_test)\n",
    "\n",
    "    elif model == \"Decision Tree\":\n",
    "        DTC = DecisionTreeClassifier(min_samples_split=7, random_state=252)\n",
    "        DTC.fit(X_train, Y_train)\n",
    "\n",
    "        stats, pred, pred_proba, acc, trueNeg = report_results(\n",
    "            DTC, X_test, Y_test)\n",
    "\n",
    "    elif model == \"Neural Network\":\n",
    "        neural_network = MLPClassifier(\n",
    "            solver=\"lbfgs\", alpha=1e-5, hidden_layer_sizes=(20, 10, 10, 5), random_state=1\n",
    "        )\n",
    "        neural_network.fit(X_train, Y_train)\n",
    "\n",
    "        stats, pred, pred_proba, acc, trueNeg = report_results(\n",
    "            neural_network, X_test, Y_test)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    test_data['Predicted Class'] = pred.tolist()\n",
    "    output_data_frame['true class'] = test_data['class']\n",
    "    output_data_frame['prediced class'] = pred.tolist()\n",
    "    output_data_frame['score assigned by model'] = pred_proba.tolist()\n",
    "    test_data['score'] = pred_proba.tolist()\n",
    "    # print(test_data['score'])\n",
    "    test_data['class'].replace(0, \"no\", inplace=True)\n",
    "    test_data['class'].replace(1, \"yes\", inplace=True)\n",
    "    test_data['Predicted Class'].replace(0, \"no\", inplace=True)\n",
    "    test_data['Predicted Class'].replace(1, \"yes\", inplace=True)\n",
    "    #print(test_data['Predicted Class'])\n",
    "\n",
    "    output_data_frame['true class'].replace(0, \"no\", inplace=True)\n",
    "    output_data_frame['true class'].replace(1, \"yes\", inplace=True)\n",
    "    output_data_frame['prediced class'].replace(0, \"no\", inplace=True)\n",
    "    output_data_frame['prediced class'].replace(1, \"yes\", inplace=True)\n",
    "\n",
    "    output_data_frame['model'] = model\n",
    "    output_data_frame['docVec'] = doc_vector\n",
    "    output_data_frame['acc'] = acc\n",
    "    output_data_frame['trueNeg'] = trueNeg\n",
    "\n",
    "    return stats, test_data, output_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a1271a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_data_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-bf16f6854adb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_data_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'output_data_frame' is not defined"
     ]
    }
   ],
   "source": [
    "print(output_data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b402e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats, test_data, output_data_frame = output_to_results(\"C:/Users/su/Desktop/Major_Project_work/data/training.csv\",\"C:/Users/su/Desktop/Major_Project_work/data/test.csv\", \"TF-IDF\", \"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = \"C:/Users/su/Desktop/\"\n",
    "model_selection = \"decision tree TF-idf neghandling lemmitization1\"\n",
    "ext = \".csv\"\n",
    "file_name = path_to_save + model_selection + ext\n",
    "\n",
    "export = output_data_frame.to_csv(r''+file_name, index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf1131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3e3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6adb52d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
